{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINI-BATCH K-MEANS: STOCHASTIC OPTIMIZATION\n",
    "\n",
    "FOR VERY LARGE DATASETS, STANDARD K-MEANS IS COMPUTATIONALLY EXPENSIVE BECAUSE IT REQUIRES CALCULATING DISTANCES TO ALL DATA POINTS IN EVERY ITERATION.\n",
    "\n",
    "**MINI-BATCH K-MEANS** SOLVES THIS BY UPDATING CENTROIDS USING SMALL, RANDOM SUBSETS (BATCHES) OF THE DATA AT EACH STEP.\n",
    "\n",
    "### ALGORITHM\n",
    "1.  **INITIALIZE** CENTROIDS.\n",
    "2.  **LOOP** FOR A FIXED NUMBER OF ITERATIONS:\n",
    "    * SAMPLE A RANDOM BATCH $B$ OF SIZE $b$.\n",
    "    * ASSIGN POINTS IN $B$ TO NEAREST CENTROIDS.\n",
    "    * UPDATE CENTROIDS USING A PER-CENTER LEARNING RATE.\n",
    "\n",
    "### UPDATE RULE (MOVING AVERAGE)\n",
    "INSTEAD OF RE-CALCULATING THE MEAN FROM SCRATCH, WE UPDATE THE CENTROID $C$ AS A RUNNING AVERAGE:\n",
    "\n",
    "$$C_{new} = (1 - \\eta) C_{old} + \\eta x$$\n",
    "\n",
    "WHERE $\\eta$ (LEARNING RATE) IS TYPICALLY THE INVERSE OF THE COUNT OF POINTS ASSIGNED TO THAT CLUSTER SO FAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVES:\n",
    "- REDUCE COMPUTATIONAL COST\n",
    "- ENABLE SCALABILITY TO LARGE DATASETS\n",
    "- MAINTAIN APPROXIMATE CLUSTER QUALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS & SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZATION CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBRARIES LOADED. VISUALIZATION STYLE CONFIGURED.\n"
     ]
    }
   ],
   "source": [
    "# SETTING GLOBAL PARAMS TO ENSURE ALL PLOTS FOLLOW THE REQUIRED STYLE\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.linestyle'] = '--'\n",
    "plt.rcParams['grid.alpha'] = 0.7\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "\n",
    "def enforce_bold_ticks(ax):\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION TO ENSURE TICKS ARE BOLD.\n",
    "    \"\"\"\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "print(\"LIBRARIES LOADED. VISUALIZATION STYLE CONFIGURED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING AND EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "FILE_PATH = 'DATA/DATA.csv'\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# EXTRACT FEATURES AS NUMPY ARRAY\n",
    "X_raw = df[['x1', 'x2']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    IMPLEMENTS Z-SCORE NORMALIZATION.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        COMPUTES MEAN AND STD, THEN SCALES THE DATA.\n",
    "        \"\"\"\n",
    "        \n",
    "        # CALCULATE MEAN AND STD ALONG AXIS 0 (COLUMNS)\n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        self.std = np.std(X, axis = 0)\n",
    "        \n",
    "        # PREVENT DIVISION BY ZERO\n",
    "        eps = 1e-15\n",
    "        self.std[self.std < eps] = eps\n",
    "        \n",
    "        return (X - self.mean) / self.std\n",
    "    \n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        REVERTS SCALING TO ORIGINAL SPACE FOR VISUALIZATION.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (X_scaled * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY SCALING\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    K-MEANS CLUSTERING IMPLEMENTATION FROM SCRATCH.\n",
    "    \n",
    "    ATTRIBUTES:\n",
    "    -----------\n",
    "    CENTROIDS : NUMPY ARRAY\n",
    "        COORDINATES OF CLUSTER CENTERS.\n",
    "    LABELS : NUMPY ARRAY\n",
    "        CLUSTER INDEX FOR EACH DATA POINT.\n",
    "    INERTIA_HISTORY : LIST\n",
    "        STORES LOSS (WCSS) AT EACH ITERATION.\n",
    "    FINAL_INERTIA : FLOAT\n",
    "        FINAL LOSS VALUE AFTER CONVERGENCE.\n",
    "    ITERATIONS_RUN : INT\n",
    "        NUMBER OF ITERATIONS EXECUTED.\n",
    "    \"\"\"\n",
    "    def __init__(self, k = 3, max_iter = 100, tol = 1e-4, metric = 'Euclidean', random_state = 36):\n",
    "        \"\"\"\n",
    "        INITIALIZES HYPERPARAMETERS.\n",
    "        \n",
    "        ARGS:\n",
    "        -----\n",
    "        K : INT\n",
    "            NUMBER OF CLUSTERS.\n",
    "        MAX_ITER : INT\n",
    "            MAXIMUM OPTIMIZATION LOOPS.\n",
    "        TOL : FLOAT\n",
    "            CONVERGENCE TOLERANCE (EPSILON).\n",
    "        METRIC : STR\n",
    "            DISTANCE METRIC ('EUCLIDEAN', 'MANHATTAN', 'COSINE').\n",
    "        RANDOM_STATE : INT\n",
    "            SEED FOR REPRODUCIBILITY.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.metric = metric\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.inertia_history = []\n",
    "        self.final_inertia = None\n",
    "        self.iterations_run = 0\n",
    "    \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        RANDOMLY SELECTS K DATA POINTS AS INITIAL CENTROIDS.\n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        \n",
    "        return X[indices[:self.k]]\n",
    "    \n",
    "    def _euclidean_distance(self, X, centroid):\n",
    "        \"\"\"\n",
    "        EUCLIDEAN DISTANCE: ||x - y||_2\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.sqrt(np.sum((X - centroid) ** 2, axis = 1))\n",
    "\n",
    "    def _manhattan_distance(self, X, centroid):\n",
    "        \"\"\"\n",
    "        MANHATTAN DISTANCE: ||x - y||_1\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.sum(np.abs(X - centroid), axis=1)\n",
    "\n",
    "    def _cosine_distance(self, X, centroid):\n",
    "        \"\"\"\n",
    "        COSINE DISTANCE = 1 - COSINE SIMILARITY\n",
    "        \"\"\"\n",
    "        \n",
    "        norm_X = np.linalg.norm(X, axis=1)\n",
    "        norm_c = np.linalg.norm(centroid)\n",
    "                \n",
    "        # AVOID DIVISION BY ZERO\n",
    "        denominator = norm_X * norm_c\n",
    "        denominator[denominator == 0] = 1e-15\n",
    "                \n",
    "        dot_product = np.dot(X, centroid)\n",
    "        similarity = dot_product / denominator\n",
    "        \n",
    "        return 1 - similarity\n",
    "    \n",
    "    def compute_distance(self, X, centroids):\n",
    "        \"\"\"\n",
    "        COMPUTES DISTANCE MATRIX BETWEEN ALL POINTS X AND ALL CENTROIDS.\n",
    "        RETURNS MATRIX OF SHAPE (N_SAMPLES, K).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_centroids = centroids.shape[0]\n",
    "        distances = np.zeros((n_samples, n_centroids))\n",
    "        \n",
    "        for i in range(n_centroids):\n",
    "            centroid = centroids[i]\n",
    "            \n",
    "            if self.metric == 'Euclidean':\n",
    "                # EUCLIDEAN: SQRT OF SUM OF SQUARED DIFFERENCES\n",
    "                \n",
    "                dist = self._euclidean_distance(X, centroid)\n",
    "            \n",
    "            elif self.metric == 'Manhattan':\n",
    "                # MANHATTAN: SUM OF ABSOLUTE DIFFERENCES\n",
    "                \n",
    "                dist = self._manhattan_distance(X, centroid) \n",
    "                \n",
    "            elif self.metric == 'Cosine':\n",
    "                # COSINE DISTANCE: 1 - COSINE SIMILARITY\n",
    "                \n",
    "                dist = self._manhattan_distance(X, centroid)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"UNSUPPORTED DISTANCE METRIC\")    \n",
    "                \n",
    "            distances[:, i] = dist\n",
    "            \n",
    "        return distances\n",
    "    \n",
    "    def assign_clusters(self, X):\n",
    "        \"\"\"\n",
    "        ASSIGN EACH POINT TO NEAREST CENTROID.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.centroids is None:\n",
    "            raise ValueError(\"MODEL HAS NOT BEEN FITTED YET!\")\n",
    "        \n",
    "        distances = self.compute_distance(X, self.centroids)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "        return labels\n",
    "\n",
    "    def update_centroids(self, X):\n",
    "        \"\"\"\n",
    "        UPDATE CENTROIDS AS MEAN OF ASSIGNED POINTS.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_centroids = np.zeros_like(self.centroids)\n",
    "\n",
    "        for cluster_idx in range(self.k):\n",
    "            cluster_points = X[self.labels == cluster_idx]\n",
    "            \n",
    "            if len(cluster_points) > 0:\n",
    "                    new_centroids[cluster_idx] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            else:\n",
    "                # HANDLE ORPHAN CENTROIDS BY KEEPING THEM STATIC\n",
    "                new_centroids[cluster_idx] = self.centroids[cluster_idx]\n",
    "                \n",
    "        return new_centroids        \n",
    "    \n",
    "    def compute_inertia(self, X, centroids, labels):\n",
    "        \"\"\"\n",
    "        CALCULATES INERTIA (WITHIN-CLUSTER SUM OF SQUARES).\n",
    "        STRICTLY USES SQUARED EUCLIDEAN DISTANCE FOR COST DEFINITION.\n",
    "        \"\"\"\n",
    "        \n",
    "        inertia = 0.0\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            cluster_points = X[labels == i]\n",
    "            \n",
    "            if len(cluster_points) > 0:\n",
    "                diff = cluster_points - centroids[i]\n",
    "                sq_dist = np.sum(diff**2)\n",
    "                inertia += sq_dist\n",
    "        \n",
    "        return inertia\n",
    "    \n",
    "    def train(self, X):\n",
    "        \"\"\"\n",
    "        TRAINING LOOP (EXPECTATION-MAXIMIZATION).\n",
    "        \"\"\"   \n",
    "        \n",
    "        start_time = time.time()                 \n",
    "        \n",
    "        # STEP 1: INITIALIZATION\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    "        self.inertia_history = []\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            self.iterations_run = i + 1\n",
    "            \n",
    "            # STEP 2: ASSIGNMENT (EXPECTATION STEP)\n",
    "            self.labels = self.assign_clusters(X)\n",
    "            \n",
    "            # SAVE OLD CENTROIDS FOR CONVERGENCE CHECK\n",
    "            old_centroids = self.centroids.copy()\n",
    "            \n",
    "            # STEP 3: UPDATE (MAXIMIZATION STEP)\n",
    "            new_centroids = self.update_centroids(X)\n",
    "            \n",
    "            self.centroids = new_centroids\n",
    "            \n",
    "            # STEP 4: RECORD LOSS\n",
    "            current_inertia = self.compute_inertia(X, self.centroids, self.labels)\n",
    "            self.inertia_history.append(current_inertia)\n",
    "            \n",
    "            # STEP 5: CONVERGENCE CHECK\n",
    "            centroid_shift = np.linalg.norm(self.centroids - old_centroids)\n",
    "            if centroid_shift < self.tol:\n",
    "                print(f\"CONVERGED AT ITERATION {i+1}\")\n",
    "                break\n",
    "        \n",
    "        self.final_inertia = self.inertia_history[-1]   \n",
    "        self.training_time = time.time() - start_time\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ASSIGN CLUSTERS TO NEW DATA.\n",
    "        \"\"\"\n",
    "        return self.assign_clusters(X)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED AT ITERATION 4\n"
     ]
    }
   ],
   "source": [
    "# DEFINE HYPERPARAMETERS\n",
    "K_CLUSTERS = 3\n",
    "MAX_ITERATIONS = 100\n",
    "TOLERANCE = 1e-4\n",
    "METRIC_TYPE = 'Euclidean'\n",
    "SEED = 36\n",
    "\n",
    "# INSTANTIATE AND TRAIN\n",
    "model = KMeans(k=K_CLUSTERS, max_iter=MAX_ITERATIONS, tol=TOLERANCE, metric=METRIC_TYPE, random_state=SEED)\n",
    "\n",
    "model.train(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORMANCE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATIONS TO CONVERGENCE: 4\n",
      "FINAL INERTIA: 19.020530\n",
      "TRAINING TIME (SECONDS): 0.004009\n"
     ]
    }
   ],
   "source": [
    "print(f\"ITERATIONS TO CONVERGENCE: {model.iterations_run}\")\n",
    "print(f\"FINAL INERTIA: {model.final_inertia:.6f}\")\n",
    "print(f\"TRAINING TIME (SECONDS): {model.training_time:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA FOR PLOTTING (INVERSE TRANSFORM)\n",
    "X_plot = scaler.inverse_transform(X_scaled)\n",
    "centroids_plot = scaler.inverse_transform(model.centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINI-BATCH K-MEANS CLASS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchKMeans:\n",
    "    \"\"\"\n",
    "    MINI-BATCH K-MEANS IMPLEMENTATION.\n",
    "    USES STOCHASTIC UPDATES FOR SCALABILITY.\n",
    "    \n",
    "    ATTRIBUTES:\n",
    "    -----------\n",
    "    CENTROIDS : NUMPY ARRAY\n",
    "        CURRENT CLUSTER CENTERS.\n",
    "    COUNTS : NUMPY ARRAY\n",
    "        NUMBER OF POINTS ASSIGNED TO EACH CLUSTER OVER TIME.\n",
    "    INERTIA_HISTORY : LIST\n",
    "        LOSS ESTIMATES AT EACH STEP.\n",
    "    \n",
    "    PARAMETERS:\n",
    "    -----------\n",
    "    k : INT\n",
    "        NUMBER OF CLUSTERS\n",
    "    batch_size : INT\n",
    "        NUMBER OF SAMPLES PER MINI-BATCH\n",
    "    max_iter : INT\n",
    "        NUMBER OF MINI-BATCH UPDATES\n",
    "    tol : FLOAT\n",
    "        CONVERGENCE THRESHOLD\n",
    "    random_seed : INT\n",
    "        RANDOM INITIALIZATION SEED    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, batch_size=10, max_iter=100, tol=1e-4, random_state=36):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "        -----\n",
    "        K : INT\n",
    "            NUMBER OF CLUSTERS.\n",
    "        BATCH_SIZE : INT\n",
    "            SIZE OF RANDOM SUBSETS USED FOR UPDATES.\n",
    "        MAX_ITER : INT\n",
    "            NUMBER OF BATCH UPDATES TO PERFORM.\n",
    "        TOL : FLOAT\n",
    "            CONVERGENCE TOLERANCE (BASED ON CENTROID MOVEMENT).\n",
    "        RANDOM_STATE : INT\n",
    "            SEED FOR REPRODUCIBILITY.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.centroids = None\n",
    "        self.counts = None\n",
    "        self.inertia_history = []\n",
    "        self.iterations_run = 0\n",
    "        \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        RANDOMLY SELECTS K DATA POINTS AS INITIAL CENTROIDS.\n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        return X[indices[:self.k]]\n",
    "    \n",
    "    def euclidean_distance(self, X, centroid):\n",
    "        \"\"\"\n",
    "        EUCLIDEAN DISTANCE: ||x - y||_2\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.sqrt(np.sum((X - centroid) ** 2, axis = 1))\n",
    "    \n",
    "    def compute_distance(self, X, centroids):\n",
    "        \"\"\"\n",
    "        COMPUTES EUCLIDEAN DISTANCE (VECTORIZED).\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_centroids = centroids.shape[0]\n",
    "        distances = np.zeros((n_samples, n_centroids))\n",
    "\n",
    "        for i in range(n_centroids):\n",
    "            centroid = centroids[i]\n",
    "            dist = self.euclidean_distance(X, centroid)\n",
    "            distances[:, i] = dist\n",
    "            \n",
    "        return distances\n",
    "    \n",
    "    def assign_clusters(self, X):\n",
    "        \"\"\"\n",
    "        ASSIGN EACH MINI-BATCH POINT TO NEAREST CENTROID.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.centroids is None:\n",
    "            raise ValueError(\"MODEL HAS NOT BEEN FITTED YET!\")\n",
    "        \n",
    "        distances = self.compute_distance(X, self.centroids)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def update_centroids(self, X_batch, labels):\n",
    "        \"\"\"\n",
    "        UPDATE CENTROIDS USING STOCHASTIC AVERAGING.\n",
    "\n",
    "        FORMULA:\n",
    "        c_new = c_old + (1 / n_c) * (x - c_old)\n",
    "        \"\"\"\n",
    "     \n",
    "        # UPDATE CENTROIDS\n",
    "        for j in range(self.k):\n",
    "            # FIND POINTS IN BATCH BELONGING TO CLUSTER J\n",
    "            cluster_points = X_batch[labels == j]\n",
    "                \n",
    "            if len(cluster_points) > 0:\n",
    "                # COUNT POINTS IN THIS BATCH FOR THIS CLUSTER\n",
    "                count_batch = len(cluster_points)\n",
    "                mean_batch = np.mean(cluster_points, axis = 0)\n",
    "                    \n",
    "                # UPDATE GLOBAL COUNT\n",
    "                self.counts[j] += count_batch\n",
    "                    \n",
    "                # CALCULATE LEARNING RATE (ETA)\n",
    "                # ETA = N_BATCH / N_TOTAL_SO_FAR\n",
    "                # THIS IS EQUIVALENT TO UPDATING THE RUNNING MEAN\n",
    "                    \n",
    "                learning_rate = count_batch / self.counts[j]\n",
    "                    \n",
    "                # UPDATE CENTROID: (1-eta)*OLD + eta*NEW\n",
    "                self.centroids[j] = (1 - learning_rate) * self.centroids[j] + learning_rate * mean_batch   \n",
    "\n",
    "    def compute_inertia(self, X_batch, labels):\n",
    "        \"\"\"\n",
    "        COMPUTE FULL DATASET INERTIA FOR MONITORING.\n",
    "        \"\"\"\n",
    "        # TRACK APPROXIMATE INERTIA (ON BATCH)\n",
    "        # CALCULATING EXACT INERTIA ON FULL DATA IS TOO SLOW FOR MINI-BATCH\n",
    "        batch_inertia = 0\n",
    "        for j in range(self.k):\n",
    "            cluster_points = X_batch[labels == j]\n",
    "            if len(cluster_points) > 0:\n",
    "                batch_inertia += np.sum((cluster_points - self.centroids[j])**2)\n",
    "        \n",
    "        self.inertia_history.append(batch_inertia)\n",
    "\n",
    "    \n",
    "    def train(self, X):\n",
    "        \"\"\"\n",
    "        TRAINS THE MODEL USING MINI-BATCH STOCHASTIC UPDATES.\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # INITIALIZATION\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    "        self.counts = np.zeros(self.k)\n",
    "        self.inertia_history = []\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # STOCHASTIC OPTIMIZATION LOOP\n",
    "        for i in range(self.max_iter):\n",
    "            self.iterations_run = i + 1\n",
    "            \n",
    "            # MANUAL BATCH SAMPLING\n",
    "            # GENERATE RANDOM INDICES\n",
    "            \n",
    "            indices = np.random.choice(n_samples, self.batch_size, replace = False)\n",
    "            X_batch = X[indices]\n",
    "            \n",
    "            labels = self.assign_clusters(X_batch)\n",
    "            \n",
    "            # STORE OLD CENTROIDS FOR CONVERGENCE CHECK\n",
    "            old_centroids = self.centroids.copy()\n",
    "            \n",
    "            self.update_centroids(X_batch, labels)\n",
    "                    \n",
    "            self.compute_inertia(X_batch, labels)\n",
    "\n",
    "            # CONVERGENCE CHECK\n",
    "            shift = np.linalg.norm(self.centroids - old_centroids)\n",
    "            \n",
    "            if shift < self.tol:\n",
    "                print(f\"MINI-BATCH CONVERGED AT ITERATION {i}\")\n",
    "                break\n",
    "            \n",
    "        self.training_time = time.time() - start_time    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ASSIGNS NEW POINTS TO FINAL CENTROIDS.\n",
    "        \"\"\"\n",
    "        \n",
    "        distances = self._compute_distance(X, self.centroids)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
