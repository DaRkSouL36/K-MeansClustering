# K-MEANS CLUSTERING FROM SCRATCH

<p align="center">
  <img src="https://img.shields.io/github/last-commit/DaRkSouL36/K-Means-Clustering?style=for-the-badge&color=blueviolet" />
  <img src="https://img.shields.io/github/repo-size/DaRkSouL36/K-Means-Clustering?style=for-the-badge&color=blue" />
  <img src="https://img.shields.io/github/languages/count/DaRkSouL36/K-Means-Clustering?style=for-the-badge&color=green" />
  <img src="https://img.shields.io/github/languages/top/DaRkSouL36/K-Means-Clustering?style=for-the-badge&color=orange" />
  <img src="https://img.shields.io/github/license/DaRkSouL36/K-Means-Clustering?style=for-the-badge&color=red" />
  <br><br>

<h1 align="center">UNSUPERVISED LEARNING. DEMYSTIFIED.</h1>

> <p align="center"><b>DISSECTING THE GEOMETRY OF DATA WITHOUT BLACK BOX LIBRARIES.</b></p>
> <p align="center"><b>IMPLEMENTED. VISUALIZED. PROVEN.</b></p>

<div style="text-align: justify;">
THIS REPOSITORY IS AN IMPLEMENTATION OF K-MEANS CLUSTERING FROM SCRATCH, BUILT ENTIRELY FROM FIRST PRINCIPLES USING ONLY NUMPY, PANDAS, AND MATPLOTLIB. THIS PROJECT DISCARDS HIGH-LEVEL WRAPPERS TO EXPOSE THE ITERATIVE OPTIMIZATION LOGIC, DISTANCE METRIC CALCULATIONS, AND CONVERGENCE MECHANISMS THAT DRIVE UNSUPERVISED LEARNING. IT FEATURES STANDARD, MINI-BATCH, AND SOFT (PROBABILISTIC) CLUSTERING VARIANTS.
</div>

---

## üìñ TABLE OF CONTENTS

- [K-MEANS CLUSTERING FROM SCRATCH](#k-means-clustering-from-scratch)
  - [üìñ TABLE OF CONTENTS](#-table-of-contents)
  - [üìò PROJECT OVERVIEW](#-project-overview)
  - [üí¨ FEATURES](#-features)
  - [üìÅ DIRECTORY STRUCTURE](#-directory-structure)
  - [üß† MATHEMATICAL CONCEPTS IMPLEMENTED](#-mathematical-concepts-implemented)
    - [1. OBJECTIVE FUNCTION (INERTIA)](#1-objective-function-inertia)
    - [2. ASSIGNMENT STEP](#2-assignment-step)
    - [3. UPDATE STEP](#3-update-step)
    - [4. SOFT CLUSTERING (BOLTZMANN)](#4-soft-clustering-boltzmann)
  - [üìä ALGORITHMS IMPLEMENTED](#-algorithms-implemented)
    - [üî∑ CLUSTERING VARIANTS](#-clustering-variants)
    - [üìê DISTANCE METRICS](#-distance-metrics)
    - [üìè VALIDATION METRICS](#-validation-metrics)
  - [üìä VISUALIZATION HIGHLIGHTS](#-visualization-highlights)
  - [‚öôÔ∏è HOW IT WORKS](#Ô∏è-how-it-works)
  - [üèÅ USAGE](#-usage)
  - [üìà RESULTS AND VALIDATION](#-results-and-validation)
    - [‚úî KEY OBSERVATIONS](#-key-observations)
  - [üöÄ FUTURE WORK](#-future-work)
  - [üìÑ LICENSE](#-license)
  - [üì¶ INSTALLATION INSTRUCTIONS](#-installation-instructions)

---

## üìò PROJECT OVERVIEW

THIS PROJECT WAS BUILT WITH A SINGLE CORE GOAL:
___

> **TO COMPLETELY REMOVE THE ABSTRACTION AROUND CLUSTERING ALGORITHMS AND EXPOSE THEIR TRUE MATHEMATICAL AND ALGORITHMIC NATURE.**

---
THIS REPOSITORY IS A DEEP DIVE INTO **UNSUPERVISED LEARNING**. UNLIKE SUPERVISED MODELS, CLUSTERING REQUIRES DISCOVERING HIDDEN STRUCTURES IN UNLABELED DATA. THIS PROJECT MANUALLY IMPLEMENTS THE **EXPECTATION-MAXIMIZATION (E-M)** LOOP, HANDLES VARIOUS GEOMETRIC DISTANCE CALCULATIONS, AND INTRODUCES STOCHASTIC OPTIMIZATION VIA MINI-BATCHING. IT BENCHMARKS ALL CUSTOM IMPLEMENTATIONS AGAINST `SCIKIT-LEARN` TO ENSURE MATHEMATICAL ACCURACY.

---

## üí¨ FEATURES

- **PURE PYTHON & NUMPY:** NO `SKLEARN` FOR CORE LOGIC. ALL CALCULATIONS ARE VECTORIZED.
- **THREE ALGORITHMS:** STANDARD K-MEANS, MINI-BATCH (FAST), AND SOFT K-MEANS (FUZZY).
- **CUSTOM DISTANCE METRICS:** EUCLIDEAN ($L_2$), MANHATTAN ($L_1$), AND COSINE DISTANCE.
- **MANUAL SCALING:** Z-SCORE NORMALIZATION IMPLEMENTED FROM SCRATCH.
- **ROBUST VISUALIZATION:** PROFESSIONAL PLOTS FOR ELBOW METHOD, LOSS CURVES, AND CLUSTER BOUNDARIES.

---

## üìÅ DIRECTORY STRUCTURE

- **K-meanCluster.ipynb** ‚Äì THE MAIN NOTEBOOK CONTAINING THE STANDARD `KMeans` CLASS, ELBOW METHOD, AND DISTANCE METRIC COMPARISONS.
- **K-meanCluster_MiniBatch.ipynb** ‚Äì ADVANCED IMPLEMENTATIONS INCLUDING `MiniBatchKMeans` (STOCHASTIC) AND `SoftKMeans` (PROBABILISTIC).
- **ANALYSIS.md** ‚Äì A DETAILED RESEARCH SUMMARY COMPARING SPEED, ACCURACY, AND CONVERGENCE ACROSS METHODS.
- **DATA/** ‚Äì CONTAINS THE RAW UNSUPERVISED DATASETS.

---

## üß† MATHEMATICAL CONCEPTS IMPLEMENTED

### 1. OBJECTIVE FUNCTION (INERTIA)
$$J = \sum_{j=1}^{K} \sum_{x_i \in C_j} ||x_i - \mu_j||^2$$

### 2. ASSIGNMENT STEP
$$C^{(t)} = \{x_p : ||x_p - \mu_i^{(t)}|| \le ||x_p - \mu_j^{(t)}|| \forall j, 1 \le j \le k\}$$

### 3. UPDATE STEP
$$\mu_i^{(t+1)} = \frac{1}{|C_i^{(t)}|} \sum_{x_j \in C_i^{(t)}} x_j$$

### 4. SOFT CLUSTERING (BOLTZMANN)
$$r_{ik} = \frac{e^{-\beta ||x_i - \mu_k||^2}}{\sum_{j=1}^K e^{-\beta ||x_i - \mu_j||^2}}$$

---

## üìä ALGORITHMS IMPLEMENTED

### üî∑ CLUSTERING VARIANTS
- **STANDARD K-MEANS:** LLOYD'S ALGORITHM FOR EXACT CONVERGENCE.
- **MINI-BATCH K-MEANS:** STOCHASTIC UPDATES FOR LARGE DATASETS AND SPEED.
- **SOFT K-MEANS:** PROBABILISTIC ASSIGNMENTS WITH TEMPERATURE PARAMETER ($\beta$).

### üìê DISTANCE METRICS
- **EUCLIDEAN:** STANDARD STRAIGHT-LINE DISTANCE (SPHERICAL CLUSTERS).
- **MANHATTAN:** GRID-BASED DISTANCE (DIAMOND SHAPE CLUSTERS).
- **COSINE:** ANGULAR DISTANCE (MAGNITUDE INDEPENDENT).

### üìè VALIDATION METRICS
- **INERTIA (WCSS):** SUM OF SQUARED ERRORS.
- **SILHOUETTE SCORE:** MEASURE OF SEPARATION AND COHESION.
- **ELBOW METHOD:** AUTOMATIC K SELECTION VISUALIZATION.

---

## üìä VISUALIZATION HIGHLIGHTS



- **VORONOI-STYLE BOUNDARIES:** VISUALIZING HOW SPACE IS PARTITIONED.
- **LOSS CURVES:** COMPARING SMOOTH DESCENT (STANDARD) VS NOISY OSCILLATION (MINI-BATCH).
- **SOFT PROBABILITIES:** COLOR OPACITY MAPPING TO SHOW CLUSTER UNCERTAINTY.
- **ELBOW CURVE:** IDENTIFYING THE OPTIMAL $K$ THROUGH INERTIA PLOTTING.

---

## ‚öôÔ∏è HOW IT WORKS

1.  **PREPROCESSING:** DATA IS LOADED AND SCALED USING MANUAL Z-SCORE NORMALIZATION ($Z = \frac{X-\mu}{\sigma}$).
2.  **INITIALIZATION:** CENTROIDS ARE RANDOMLY SEEDED (OR VIA K-MEANS++ LOGIC).
3.  **ITERATION:**
    * **CALCULATE DISTANCES:** VECTORIZED OPERATIONS AGAINST ALL CENTROIDS.
    * **ASSIGN:** POINTS MOVE TO NEAREST CLUSTER (HARD) OR DISTRIBUTE WEIGHTS (SOFT).
    * **UPDATE:** CENTROIDS SHIFT TO THE NEW MEAN OF THEIR ASSIGNED POINTS.
4.  **CONVERGENCE:** LOOP STOPS WHEN CENTROID SHIFT < TOLERANCE ($\epsilon$).

---

## üèÅ USAGE

1.  **LOAD DATA:** ENSURE `DATA.csv` IS IN THE DIRECTORY.
2.  **INSTANTIATE CLASS:**
    ```python
    model = KMeans(k=3, max_iter=100, metric='euclidean')
    model.fit(X_scaled)
    ```
3.  **VISUALIZE:** CALL THE PLOTTING UTILITIES TO SEE CLUSTERS AND CENTROIDS.

---

## üìà RESULTS AND VALIDATION

<div style="text-align: justify;">
THE IMPLEMENTATION WAS RIGOROUSLY TESTED AGAINST SCIKIT-LEARN. THE COMPARISON CONFIRMED THAT THE FIRST-PRINCIPLES APPROACH YIELDS IDENTICAL MATHEMATICAL RESULTS.

- **CENTROIDS:** THE CUSTOM IMPLEMENTATION CONVERGED TO THE EXACT SAME COORDINATES AS SKLEARN.
- **INERTIA:** FINAL LOSS VALUES MATCHED TO 4 DECIMAL PLACES.
- **MINI-BATCH:** DEMONSTRATED A 10X SPEEDUP POTENTIAL WITH MARGINAL LOSS IN ACCURACY.
</div>

### ‚úî KEY OBSERVATIONS
- **EUCLIDEAN** DISTANCE PERFORMED BEST FOR SPHERICAL DATA CLUSTERS.
- **MINI-BATCH** IS PREFERRED FOR $N > 10,000$ SAMPLES.
- **SOFT K-MEANS** REVEALED BOUNDARY AMBIGUITY HIDDEN BY HARD ASSIGNMENTS.

---

## üöÄ FUTURE WORK

- IMPLEMENT **K-MEANS++** INITIALIZATION FOR BETTER CONVERGENCE GUARANTEES.
- ADD **GAUSSIAN MIXTURE MODELS (GMM)** FOR FULL COVARIANCE MODELING.
- INTERACTIVE VISUAL DASHBOARD
- IMPLEMENT **DBSCAN** FOR DENSITY-BASED CLUSTERING (NON-SPHERICAL).

---

## üìÑ LICENSE

THIS PROJECT IS LICENSED UNDER THE [MIT LICENSE](LICENSE).

YOU ARE FREE TO USE, MODIFY, AND DISTRIBUTE THIS PROJECT WITH ATTRIBUTION.

---

## üì¶ INSTALLATION INSTRUCTIONS

> 1. **CLONE THE REPOSITORY**

---

`git clone https://github.com/DaRkSouL36/K-MeansClustering`

`cd "K-MeansClustering"`

---

> 2. **CREATE VIRTUAL ENVIRONMENT**

---

`python -m venv VENV`

- LINUX/MAC
  
   `source VENV/bin/activate`

- ON WINDOWS
  
   `VENV\Scripts\activate`

---

> 3. **INSTALL DEPENDENCIES** ---

`pip install -r REQUIREMENTS.txt`

---

<p>
  <a href="#k-means-clustering-from-scratch">
    <img src="https://img.icons8.com/fluency/48/up.png" width="20px"/>
    <strong>RETURN</strong>
  </a>
</p>